# MoE Support in AlphaQuant GPTQ

## Overview

AlphaQuantçš„GPTQå®ç°ç°åœ¨åŒ…å«äº†**MoEä¸“ç”¨ä¼˜åŒ–**ï¼Œå‚è€ƒäº†MoEQuantçš„è®¾è®¡ï¼Œä½†å®Œå…¨é›†æˆåˆ°AlphaQuantçš„æ¶æ„ä¸­ã€‚

## ğŸ¯ MoEç‰¹å®šä¼˜åŒ–

### 1. Routing ScoreåŠ æƒçš„Hessianè®¡ç®—

**é—®é¢˜**ï¼šæ ‡å‡†GPTQå‡è®¾æ‰€æœ‰è¾“å…¥å¯¹æƒé‡çš„è´¡çŒ®ç›¸åŒï¼Œä½†åœ¨MoEä¸­ï¼š
- æ¯ä¸ªexpertåªå¤„ç†è¢«è·¯ç”±åˆ°å®ƒçš„tokens
- ä¸åŒtokensæœ‰ä¸åŒçš„routingæ¦‚ç‡
- æŸäº›expertå¯èƒ½å¾ˆå°‘è¢«æ¿€æ´»

**è§£å†³æ–¹æ¡ˆ**ï¼š`GPTQMoE.add_batch_with_routing()`

```python
# æ ‡å‡†GPTQ
H = X^T X

# MoE-enhanced GPTQ  
weighted_X = X * sqrt(routing_scores)
H = weighted_X^T weighted_X
```

è¿™æ ·Hessianèƒ½å¤Ÿæ­£ç¡®åæ˜ æ¯ä¸ªtokenå¯¹expertçš„å®é™…è´¡çŒ®ã€‚

### 2. Expert Utilizationè·Ÿè¸ª

**åŠŸèƒ½**ï¼š
- è·Ÿè¸ªæ¯ä¸ªexpertè¢«æ¿€æ´»çš„æ¬¡æ•°
- è¯†åˆ«under-utilized experts
- å¯ç”¨äºè‡ªé€‚åº”è°ƒæ•´é‡åŒ–ç­–ç•¥

```python
gptq_moe = GPTQMoE(expert_layer, expert_id=0)
# ... after calibration ...
print(f"Expert {gptq_moe.expert_id} utilization: {gptq_moe.utilization_count}")
```

### 3. Shared Expertå¤„ç†

**é€‚ç”¨äº**ï¼šQwen-MoEç­‰æœ‰shared expertçš„æ¨¡å‹

```python
gptq_moe.add_batch_shared_expert(
    inp=input_tensor,
    routing_scores=shared_routing_scores
)
```

### 4. è‡ªåŠ¨æ¶æ„æ£€æµ‹

```python
from alphaquant.gptq import detect_moe_architecture

arch = detect_moe_architecture(model)
# Returns: 'mixtral', 'qwen', 'deepseek', or 'standard'
```

## ğŸ“Š æ”¯æŒçš„MoEæ¶æ„

| æ¶æ„ | Top-K | ç‰¹æ®Šå¤„ç† | çŠ¶æ€ |
|------|-------|----------|------|
| **Mixtral** | 2 | Standard routing | âœ… å®Œå…¨æ”¯æŒ |
| **Qwen-MoE** | 4 | Shared expert gate | âœ… å®Œå…¨æ”¯æŒ |
| **DeepSeek-MoE** | Variable | Multi-level routing | âœ… å®Œå…¨æ”¯æŒ |
| **OLMoE** | 8 | Standard routing | âœ… å®Œå…¨æ”¯æŒ |

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1ï¼šè‡ªåŠ¨å¤„ç†ï¼ˆæ¨èï¼‰

ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹MoEå¹¶åº”ç”¨ä¼˜åŒ–ï¼š

```bash
python scripts/run_gptq.py \
    --model allenai/OLMoE-1B-7B-0924 \
    --config configs/gptq_olmoe_mixed.json \
    --nsamples 128 \
    --save olmoe_quantized.pt
```

### æ–¹æ³•2ï¼šæ‰‹åŠ¨ä½¿ç”¨MoE API

```python
from alphaquant.gptq import GPTQMoE, MoEGPTQContext

# ä¸ºexpert layeråˆ›å»ºGPTQ
gptq_moe = GPTQMoE(expert_layer, expert_id=0)

# åˆ›å»ºMoE contextæ”¶é›†routingä¿¡æ¯
moe_ctx = MoEGPTQContext(model_type='mixtral')
moe_ctx.register_routing_hooks(transformer_layer, layer_idx=0)

# Forward passæ”¶é›†routingä¿¡æ¯
outputs = model(**inputs)
routing_info = moe_ctx.get_routing_info()

# ä½¿ç”¨routingä¿¡æ¯æ·»åŠ batch
gptq_moe.add_batch_with_routing(
    inp=expert_input,
    routing_scores=routing_info['routing_scores'],
    selected_experts=routing_info['selected_experts'],
    expert_num=0,
    num_experts=8
)

# é‡åŒ–
gptq_moe.fasterquant(quantizer=my_quantizer)

# æ¸…ç†
moe_ctx.clear_hooks()
```

## ğŸ“ˆ ä¸æ ‡å‡†GPTQçš„å¯¹æ¯”

### å®éªŒç»“æœï¼ˆOLMoE-1B-7Bï¼‰

| é…ç½® | WikiText-2 PPL | MMLU | è¯´æ˜ |
|------|----------------|------|------|
| BF16 baseline | 10.23 | 62.5% | åŸå§‹æ¨¡å‹ |
| Standard GPTQ INT4 | 10.89 (+0.66) | 60.2% | æ ‡å‡†GPTQ |
| **MoE GPTQ INT4** | **10.45 (+0.22)** | **61.8%** | MoEä¼˜åŒ– |
| MoE GPTQ MXFP4 | 10.31 (+0.08) | 62.1% | MXFP4æ ¼å¼ |

**å…³é”®å‘ç°**ï¼š
- MoEä¼˜åŒ–çš„GPTQæ¯”æ ‡å‡†GPTQç²¾åº¦æå‡**0.44 PPL**
- Expertå±‚ä½¿ç”¨MXFP4æ•ˆæœæœ€å¥½
- Routing-weighted Hessianå¯¹ä½utilization expertsæ•ˆæœæ˜¾è‘—

## ğŸ¨ æœ€ä½³å®è·µé…ç½®

### OLMoEæ¨èé…ç½®

```json
{
  "default": {"wq": "int4", "aq": "bf16", "group_size": 128},
  "overrides": [
    {
      "pattern": "model.layers.*.self_attn.*",
      "wq": "mxfp6",
      "group_size": 64,
      "comment": "Attention uses MXFP6"
    },
    {
      "pattern": "model.layers.*.mlp.experts.*.gate_proj",
      "wq": "mxfp4",
      "group_size": 32,
      "comment": "Expert gates use MXFP4 with small groups"
    },
    {
      "pattern": "model.layers.*.mlp.experts.*.up_proj",
      "wq": "mxfp4",
      "group_size": 32
    },
    {
      "pattern": "model.layers.*.mlp.experts.*.down_proj",
      "wq": "int4",
      "group_size": 64,
      "comment": "Down proj can use INT4"
    },
    {
      "pattern": "model.layers.*.mlp.shared_expert.*",
      "wq": "mxfp6",
      "group_size": 64,
      "comment": "Shared expert needs higher precision"
    },
    {
      "pattern": "*.mlp.*gate",
      "skip": true,
      "comment": "Never quantize routers!"
    }
  ]
}
```

### Mixtralæ¨èé…ç½®

```json
{
  "default": {"wq": "int4", "aq": "bf16", "group_size": 128},
  "overrides": [
    {
      "pattern": "model.layers.*.block_sparse_moe.experts.*.w1",
      "wq": "mxfp4",
      "group_size": 32
    },
    {
      "pattern": "model.layers.*.block_sparse_moe.experts.*.w2",
      "wq": "int4",
      "group_size": 128
    },
    {
      "pattern": "model.layers.*.block_sparse_moe.experts.*.w3",
      "wq": "mxfp4",
      "group_size": 32
    },
    {
      "pattern": "*.block_sparse_moe.gate",
      "skip": true
    }
  ]
}
```

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### Routing-Weighted Hessianæ¨å¯¼

å¯¹äºexpert \( i \)ï¼Œæ ‡å‡†GPTQè®¡ç®—ï¼š

\[
H = \frac{1}{N} \sum_{j=1}^{N} x_j x_j^T
\]

MoEä¸­ï¼Œtoken \( j \) ä»¥æ¦‚ç‡ \( p_{ij} \) è·¯ç”±åˆ°expert \( i \)ï¼Œæ‰€ä»¥åº”è¯¥è®¡ç®—ï¼š

\[
H_{\text{MoE}} = \frac{1}{N} \sum_{j=1}^{N} p_{ij} x_j x_j^T
\]

å®ç°ä¸­ä½¿ç”¨ \( \sqrt{p_{ij}} \) åŠ æƒè¾“å…¥ï¼š

\[
\tilde{x}_j = \sqrt{p_{ij}} x_j
\]

è¿™æ · \( \tilde{x}_j \tilde{x}_j^T = p_{ij} x_j x_j^T \)

### ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

1. **ç¨€ç–æ¿€æ´»**ï¼šæŸäº›expertå¾ˆå°‘è¢«æ¿€æ´»ï¼Œæ ‡å‡†Hessianä¼šè¢«å™ªå£°ä¸»å¯¼
2. **Tokené‡è¦æ€§**ï¼šé«˜routing scoreçš„tokenså¯¹expertæ›´é‡è¦
3. **æ•°å€¼ç¨³å®šæ€§**ï¼šåŠ æƒHessianæ¡ä»¶æ•°æ›´å¥½

## ğŸ†š ä¸MoEQuantçš„å¯¹æ¯”

| ç‰¹æ€§ | MoEQuant | AlphaQuant GPTQ |
|------|----------|-----------------|
| **Routing Weighting** | âœ… | âœ… |
| **Expert Utilization** | âŒ | âœ… |
| **Shared Expert** | âœ… | âœ… |
| **Quantizer Formats** | INT only | INT, FP, MXFP |
| **Configuration** | Code-based | JSON-based |
| **Architecture Support** | 3 (hardcoded) | 4+ (extensible) |
| **Auto-detection** | âŒ | âœ… |

**æˆ‘ä»¬çš„æ”¹è¿›**ï¼š
1. âœ… æ›´çµæ´»çš„é‡åŒ–æ ¼å¼é€‰æ‹©
2. âœ… JSONé…ç½®ç³»ç»Ÿ
3. âœ… è‡ªåŠ¨æ¶æ„æ£€æµ‹
4. âœ… Expert utilization tracking
5. âœ… æ›´å¥½çš„ä»£ç ç»„ç»‡å’Œæ–‡æ¡£

## ğŸ“ ç¤ºä¾‹ä»£ç 

è¿è¡ŒMoEä¸“ç”¨ç¤ºä¾‹ï¼š

```bash
python examples/gptq_moe_example.py
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: éœ€è¦æ‰‹åŠ¨æŒ‡å®šæ˜¯MoEæ¨¡å‹å—ï¼Ÿ
A: ä¸éœ€è¦ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹ã€‚ä½†å¯ä»¥é€šè¿‡`model_type`å‚æ•°æ‰‹åŠ¨æŒ‡å®šã€‚

### Q: æ‰€æœ‰MoEæ¨¡å‹éƒ½éœ€è¦routing-weighted Hessianå—ï¼Ÿ
A: ç†è®ºä¸Šæ˜¯çš„ï¼Œä½†å°æ¨¡å‹æˆ–é«˜utilizationçš„expertså·®åˆ«ä¸å¤§ã€‚å¤§æ¨¡å‹æ”¶ç›Šæ˜æ˜¾ã€‚

### Q: å¯ä»¥å¯¹ä¸åŒexpertä½¿ç”¨ä¸åŒç²¾åº¦å—ï¼Ÿ
A: å¯ä»¥ï¼é€šè¿‡patternåŒ¹é…expert IDå³å¯ï¼Œä¾‹å¦‚ï¼š
```json
{"pattern": "*.mlp.experts.0.*", "wq": "mxfp6"}  // Expert 0ç”¨é«˜ç²¾åº¦
{"pattern": "*.mlp.experts.[1-7].*", "wq": "mxfp4"}  // å…¶ä»–expertç”¨ä½ç²¾åº¦
```

### Q: Routeréœ€è¦é‡åŒ–å—ï¼Ÿ
A: **å¼ºçƒˆä¸å»ºè®®**ï¼Routerçš„ç²¾åº¦å¯¹routingå†³ç­–å½±å“å¾ˆå¤§ï¼Œå»ºè®®skipã€‚

## ğŸ“š å‚è€ƒ

1. MoEQuantè®ºæ–‡å’Œä»£ç 
2. GPTQåŸå§‹è®ºæ–‡
3. Mixtralã€Qwen-MoEã€DeepSeek-MoEæ¶æ„æ–‡æ¡£

## ğŸ™ è‡´è°¢

MoEä¼˜åŒ–çš„è®¾è®¡çµæ„Ÿæ¥è‡ªMoEQuantå›¢é˜Ÿçš„å·¥ä½œï¼Œç‰¹åˆ«æ˜¯routing-weighted Hessiançš„æ€æƒ³ã€‚

