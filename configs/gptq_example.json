{
  "default": {
    "wq": "int4",
    "aq": "bf16",
    "group_size": 128
  },
  "overrides": [
    {
      "pattern": "model.layers.*.self_attn.q_proj",
      "wq": "int4",
      "aq": "bf16",
      "group_size": 128
    },
    {
      "pattern": "model.layers.*.self_attn.k_proj",
      "wq": "int4",
      "aq": "bf16",
      "group_size": 128
    },
    {
      "pattern": "model.layers.*.self_attn.v_proj",
      "wq": "int4",
      "aq": "bf16",
      "group_size": 128
    },
    {
      "pattern": "model.layers.*.self_attn.o_proj",
      "wq": "int4",
      "aq": "bf16",
      "group_size": 128
    },
    {
      "pattern": "model.layers.*.mlp.gate_proj",
      "wq": "int4",
      "aq": "bf16",
      "group_size": 128
    },
    {
      "pattern": "model.layers.*.mlp.up_proj",
      "wq": "int4",
      "aq": "bf16",
      "group_size": 128
    },
    {
      "pattern": "model.layers.*.mlp.down_proj",
      "wq": "int4",
      "aq": "bf16",
      "group_size": 128
    }
  ]
}

