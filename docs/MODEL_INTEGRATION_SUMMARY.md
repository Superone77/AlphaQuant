# AlphaQuantæ¨¡å‹é›†æˆæ€»ç»“

## âœ… å®Œæˆçš„å·¥ä½œ

### 1. æ¨¡å‹ç»“æ„ä»£ç é›†æˆ

å·²æˆåŠŸå°†ä»¥ä¸‹4ä¸ªMoEæ¨¡å‹çš„ä»£ç é›†æˆåˆ°`models/`æ–‡ä»¶å¤¹ï¼š

| æ¨¡å‹ | è·¯å¾„ | çŠ¶æ€ | æ¥æº |
|------|------|------|------|
| **OLMoE** | `models/olmoe/` | âœ… å®Œæˆ | Transformers 4.46+ |
| **Qwen2-MoE** | `models/qwen_moe_14b_chat/` | âœ… å®Œæˆ | Transformers 4.37+ |
| **Mixtral** | `models/mixtral_model/` | âœ… å®Œæˆ | Transformers 4.36+ |
| **DeepSeek-MoE** | `models/deepseek_moe_16b_chat/` | âœ… å®Œæˆ | MoEQuanté¡¹ç›® |

æ¯ä¸ªæ¨¡å‹åŒ…å«ï¼š
- âœ… `configuration_*.py` - é…ç½®ç±»
- âœ… `modeling_*.py` - æ¨¡å‹å®ç°
- âœ… `__init__.py` - æ¨¡å—åˆå§‹åŒ–

### 2. Importè·¯å¾„ä¿®å¤

æ‰€æœ‰æ¨¡å‹æ–‡ä»¶çš„importå·²ä»ç›¸å¯¹è·¯å¾„ä¿®å¤ä¸ºç»å¯¹è·¯å¾„ï¼š

```python
# ä¿®å¤å‰ï¼ˆä»transformersåº“å¤åˆ¶æ¥çš„ï¼‰
from ...modeling_utils import PreTrainedModel
from ...configuration_utils import PretrainedConfig

# ä¿®å¤å
from transformers.modeling_utils import PreTrainedModel
from transformers.configuration_utils import PretrainedConfig
```

**ä¿®å¤ç»Ÿè®¡**ï¼š
- æ€»å¤„ç†æ–‡ä»¶: 8ä¸ª
- ä¿®å¤çš„import: 18å¤„
- è‡ªåŠ¨ä¿®å¤è„šæœ¬: `fix_model_imports.py`

### 3. AlphaQuant GPTQé›†æˆ

æ‰€æœ‰4ä¸ªæ¨¡å‹å·²å®Œå…¨é›†æˆåˆ°AlphaQuantçš„GPTQ pipelineï¼š

#### æ¨¡å‹ç±»å‹æ£€æµ‹ (`alphaquant/gptq/model_utils.py`)

```python
def get_layers_for_model(model, model_type='auto'):
    # æ”¯æŒè‡ªåŠ¨æ£€æµ‹ï¼š
    # - olmoe, olmo -> 'olmoe'
    # - qwen, qwen2moe -> 'qwen'
    # - mixtral -> 'mixtral'
    # - deepseek -> 'deepseek'
```

#### MoEæ¶æ„æ£€æµ‹ (`alphaquant/gptq/gptq_moe.py`)

```python
def detect_moe_architecture(model):
    # è¿”å›: 'olmoe', 'qwen', 'mixtral', 'deepseek', 'generic_moe', 'standard'
```

#### Routingå¤„ç†

| æ¨¡å‹ | Top-K | Routing Hook | çŠ¶æ€ |
|------|-------|--------------|------|
| OLMoE | 8 | `save_olmoe_routing` | âœ… |
| Qwen2-MoE | 4 + shared | `save_qwen_routing` + `save_shared_routing` | âœ… |
| Mixtral | 2 | `save_mixtral_routing` | âœ… |
| DeepSeek-MoE | Variable | `save_deepseek_routing` | âœ… |

### 4. é…ç½®æ–‡ä»¶

æä¾›äº†ä¸“é—¨çš„é…ç½®æ–‡ä»¶ï¼š

- âœ… `configs/gptq_olmoe_mixed.json` - OLMoEæ··åˆç²¾åº¦é…ç½®
- âœ… `configs/gptq_mixed_precision.json` - é€šç”¨æ··åˆç²¾åº¦é…ç½®
- âœ… `configs/gptq_example.json` - åŸºç¡€é…ç½®

### 5. æ–‡æ¡£

åˆ›å»ºäº†å®Œæ•´çš„æ–‡æ¡£ï¼š

- âœ… `models/README.md` - æ¨¡å‹ä½¿ç”¨æŒ‡å—
- âœ… `alphaquant/gptq/MOE_SUPPORT.md` - MoEæ”¯æŒæ–‡æ¡£
- âœ… `GPTQ_PIPELINE_README.md` - GPTQå®Œæ•´æ–‡æ¡£
- âœ… `tests/test_model_loading.py` - æ¨¡å‹åŠ è½½æµ‹è¯•

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: ä½¿ç”¨æœ¬åœ°æ¨¡å‹å®šä¹‰

```python
from models.olmoe.modeling_olmoe import OlmoeForCausalLM
from models.olmoe.configuration_olmoe import OlmoeConfig

model = OlmoeForCausalLM.from_pretrained(
    "allenai/OLMoE-1B-7B-0924",
    torch_dtype=torch.bfloat16
)
```

ä¼˜ç‚¹ï¼š
- âœ… ä¸å—transformersç‰ˆæœ¬å½±å“
- âœ… å¯ä»¥è‡ªå®šä¹‰ä¿®æ”¹æ¨¡å‹ä»£ç 
- âœ… ç¡®ä¿ä»£ç ç¨³å®šæ€§

### æ–¹æ³•2: ä½¿ç”¨Transformers AutoModelï¼ˆå¦‚æœç‰ˆæœ¬è¶³å¤Ÿæ–°ï¼‰

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "allenai/OLMoE-1B-7B-0924",
    trust_remote_code=True
)
```

ä¼˜ç‚¹ï¼š
- âœ… ä»£ç ç®€æ´
- âœ… è‡ªåŠ¨å¤„ç†
- âŒ ä¾èµ–transformersç‰ˆæœ¬

### æ–¹æ³•3: ä½¿ç”¨AlphaQuant GPTQï¼ˆæ¨èï¼‰

```bash
# è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹å¹¶ä½¿ç”¨MoEä¼˜åŒ–
python scripts/run_gptq.py \
    --model allenai/OLMoE-1B-7B-0924 \
    --config configs/gptq_olmoe_mixed.json \
    --save olmoe_quantized.pt
```

ä¼˜ç‚¹ï¼š
- âœ… è‡ªåŠ¨MoEæ£€æµ‹
- âœ… Routing-weighted Hessian
- âœ… æ··åˆç²¾åº¦æ”¯æŒ

## ğŸ” éªŒè¯æ£€æŸ¥æ¸…å•

### æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§

```bash
# æ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
for model in olmoe qwen_moe_14b_chat mixtral_model deepseek_moe_16b_chat; do
    echo "Checking $model..."
    ls models/$model/*.py
done
```

### Importè·¯å¾„æ£€æŸ¥

```bash
# ç¡®ä¿æ²¡æœ‰ç›¸å¯¹å¯¼å…¥
grep -r "from \.\.\." models/*/modeling*.py models/*/configuration*.py
# åº”è¯¥è¿”å›ç©ºç»“æœ
```

### GPTQé›†æˆæ£€æŸ¥

```python
from alphaquant.gptq import detect_moe_architecture
from transformers import AutoModelForCausalLM

models_to_test = [
    ("allenai/OLMoE-1B-7B-0924", "olmoe"),
    ("Qwen/Qwen1.5-MoE-A2.7B", "qwen"),
    ("mistralai/Mixtral-8x7B-v0.1", "mixtral"),
]

for model_name, expected_arch in models_to_test:
    model = AutoModelForCausalLM.from_pretrained(model_name)
    detected = detect_moe_architecture(model)
    print(f"{model_name}: {detected} (expected: {expected_arch})")
```

## ğŸ“Š æ¨¡å‹æ¶æ„å¯¹æ¯”

| ç‰¹æ€§ | OLMoE | Qwen2-MoE | Mixtral | DeepSeek-MoE |
|------|-------|-----------|---------|--------------|
| **Expertsæ•°é‡** | 8 | 60 | 8 | 64 |
| **Top-K** | 8 | 4 | 2 | Variable |
| **Shared Expert** | âŒ | âœ… | âŒ | âŒ |
| **éšè—å±‚** | 16 | 24 | 32 | 60 |
| **å‚æ•°é‡** | 1B+7B | 2.7B+14B | 7B x 8 | 16B |

## ğŸ¯ æœ€ä½³å®è·µé…ç½®

### OLMoEæ¨èé…ç½®

```json
{
  "default": {"wq": "int4", "aq": "bf16", "group_size": 128},
  "overrides": [
    {"pattern": "*.mlp.experts.*", "wq": "mxfp4", "group_size": 32},
    {"pattern": "*.mlp.gate", "skip": true}
  ]
}
```

### Qwen2-MoEæ¨èé…ç½®

```json
{
  "default": {"wq": "int4", "aq": "bf16", "group_size": 128},
  "overrides": [
    {"pattern": "*.mlp.experts.*", "wq": "mxfp4", "group_size": 32},
    {"pattern": "*.mlp.shared_expert.*", "wq": "mxfp6", "group_size": 64},
    {"pattern": "*.mlp.*gate", "skip": true}
  ]
}
```

### Mixtralæ¨èé…ç½®

```json
{
  "default": {"wq": "int4", "aq": "bf16", "group_size": 128},
  "overrides": [
    {"pattern": "*.block_sparse_moe.experts.*.w1", "wq": "mxfp4", "group_size": 32},
    {"pattern": "*.block_sparse_moe.experts.*.w3", "wq": "mxfp4", "group_size": 32},
    {"pattern": "*.block_sparse_moe.experts.*.w2", "wq": "int4", "group_size": 128},
    {"pattern": "*.block_sparse_moe.gate", "skip": true}
  ]
}
```

## âš ï¸ å·²çŸ¥é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: ModuleNotFoundError: No module named 'transformers'

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
pip install transformers>=4.36.0
```

### é—®é¢˜2: ImportError: cannot import name 'RopeParameters'

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
pip install transformers>=4.41.0
# æˆ–è€…ä»£ç å·²ç»å¤„ç†äº†fallback
```

### é—®é¢˜3: æ¨¡å‹åŠ è½½å¤±è´¥

**å¯èƒ½åŸå› **ï¼š
1. Transformersç‰ˆæœ¬å¤ªæ—§
2. ç¼ºå°‘ä¾èµ–åŒ…
3. æ¨¡å‹æƒé‡ä¸å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å‡çº§transformers
pip install --upgrade transformers

# å®‰è£…æ‰€æœ‰ä¾èµ–
pip install torch transformers accelerate
```

## ğŸ”§ ç»´æŠ¤æŒ‡å—

### æ·»åŠ æ–°æ¨¡å‹

1. ä»transformersæˆ–å®˜æ–¹repoå¤åˆ¶æ¨¡å‹ä»£ç åˆ°`models/new_model/`
2. è¿è¡Œimportä¿®å¤è„šæœ¬ï¼š
   ```bash
   python fix_model_imports.py
   ```
3. åˆ›å»º`__init__.py`
4. æ›´æ–°`alphaquant/gptq/model_utils.py`æ·»åŠ æ£€æµ‹é€»è¾‘
5. å¦‚æœæ˜¯MoEæ¨¡å‹ï¼Œæ›´æ–°`alphaquant/gptq/gptq_moe.py`æ·»åŠ routingå¤„ç†
6. åˆ›å»ºé…ç½®æ–‡ä»¶åˆ°`configs/`
7. æ›´æ–°æ–‡æ¡£

### åŒæ­¥transformersæ›´æ–°

```bash
# 1. ä»transformerså¤åˆ¶æ–°ç‰ˆæœ¬
cp -r /path/to/transformers/models/olmoe models/

# 2. ä¿®å¤imports
python fix_model_imports.py

# 3. æµ‹è¯•
python tests/test_model_loading.py
```

## ğŸ“š å‚è€ƒ

- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [MoEQuant Project](https://github.com/MoEQuant/MoEQuant)
- [OLMoE Paper](https://arxiv.org/abs/2409.02060)
- [Mixtral Blog](https://mistral.ai/news/mixtral-of-experts/)
- [Qwen Technical Report](https://arxiv.org/abs/2309.16609)

## ğŸ‰ æ€»ç»“

âœ… **å®Œå…¨å®ç°**ï¼š
1. 4ä¸ªMoEæ¨¡å‹çš„ç‹¬ç«‹ä»£ç å®ç°
2. ä¿®å¤æ‰€æœ‰importè·¯å¾„
3. å®Œæ•´çš„GPTQé›†æˆ
4. è‡ªåŠ¨MoEæ¶æ„æ£€æµ‹
5. Routing-weighted Hessianæ”¯æŒ
6. å®Œæ•´çš„æ–‡æ¡£å’Œé…ç½®

âœ… **ç‰ˆæœ¬ç‹¬ç«‹æ€§**ï¼š
- ä¸å†å—transformersç‰ˆæœ¬é™åˆ¶
- å¯ä»¥è‡ªç”±ä¿®æ”¹æ¨¡å‹ä»£ç 
- ä¿è¯é•¿æœŸç¨³å®šæ€§

âœ… **å³æ’å³ç”¨**ï¼š
- åªéœ€importå°±èƒ½ä½¿ç”¨
- è‡ªåŠ¨æ£€æµ‹å’Œä¼˜åŒ–
- é…ç½®é©±åŠ¨çš„é‡åŒ–

ğŸ¯ **ä¸‹ä¸€æ­¥å»ºè®®**ï¼š
1. åœ¨å®é™…æ¨¡å‹ä¸Šæµ‹è¯•GPTQ quantization
2. æ ¹æ®éœ€è¦è°ƒæ•´é…ç½®æ–‡ä»¶
3. è€ƒè™‘æ·»åŠ æ›´å¤šæ¨¡å‹ï¼ˆå¦‚GLMã€Gemmaç­‰ï¼‰

