# AlphaQuant Models

è¿™ä¸ªæ–‡ä»¶å¤¹åŒ…å«äº†ç‹¬ç«‹çš„æ¨¡å‹å®šä¹‰ï¼Œä½¿å¾—AlphaQuantä¸å—Transformersåº“ç‰ˆæœ¬çš„å½±å“ã€‚

## ğŸ“¦ åŒ…å«çš„æ¨¡å‹

### 1. OLMoE (AllenAI)
- **è·¯å¾„**: `models/olmoe/`
- **æ–‡ä»¶**:
  - `configuration_olmoe.py` - é…ç½®ç±»
  - `modeling_olmoe.py` - æ¨¡å‹å®ç°
  - `__init__.py` - æ¨¡å—åˆå§‹åŒ–
- **ç‰¹æ€§**: Top-8 routing, 8ä¸ªexpert
- **ç”¨é€”**: AllenAIçš„å¼€æºMoEæ¨¡å‹

### 2. Qwen2-MoE (Alibaba)
- **è·¯å¾„**: `models/qwen_moe_14b_chat/`
- **æ–‡ä»¶**:
  - `configuration_qwen2_moe.py` - é…ç½®ç±»
  - `modeling_qwen2_moe.py` - æ¨¡å‹å®ç°
  - `__init__.py` - æ¨¡å—åˆå§‹åŒ–
- **ç‰¹æ€§**: Top-4 routing + Shared Expert
- **ç”¨é€”**: Qwenç³»åˆ—çš„MoEæ¨¡å‹

### 3. Mixtral (Mistral AI)
- **è·¯å¾„**: `models/mixtral_model/`
- **æ–‡ä»¶**:
  - `configuration_mixtral.py` - é…ç½®ç±»
  - `modeling_mixtral.py` - æ¨¡å‹å®ç°
  - `__init__.py` - æ¨¡å—åˆå§‹åŒ–
- **ç‰¹æ€§**: Top-2 routing, sparse MoE
- **ç”¨é€”**: Mixtral 7B x 8 Expertæ¨¡å‹

### 4. DeepSeek-MoE (DeepSeek)
- **è·¯å¾„**: `models/deepseek_moe_16b_chat/`
- **æ–‡ä»¶**:
  - `configuration_deepseek.py` - é…ç½®ç±»
  - `modeling_deepseek.py` - æ¨¡å‹å®ç°
  - `__init__.py` - æ¨¡å—åˆå§‹åŒ–
- **ç‰¹æ€§**: Multi-level routing, 64 experts
- **ç”¨é€”**: DeepSeekçš„å¤§è§„æ¨¡MoEæ¨¡å‹

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: ç›´æ¥å¯¼å…¥

```python
from models.olmoe.modeling_olmoe import OlmoeForCausalLM
from models.olmoe.configuration_olmoe import OlmoeConfig

# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ç±»
config = OlmoeConfig.from_pretrained("allenai/OLMoE-1B-7B-0924")
model = OlmoeForCausalLM.from_pretrained("allenai/OLMoE-1B-7B-0924")
```

### æ–¹æ³•2: ä½¿ç”¨AutoModel (æ¨è)

å¦‚æœtransformersç‰ˆæœ¬å·²ç»æ”¯æŒè¯¥æ¨¡å‹ï¼š

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "allenai/OLMoE-1B-7B-0924",
    trust_remote_code=True
)
```

### æ–¹æ³•3: æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹åˆ°AutoModel

```python
from transformers import AutoConfig, AutoModelForCausalLM
from models.olmoe.configuration_olmoe import OlmoeConfig
from models.olmoe.modeling_olmoe import OlmoeForCausalLM

# æ³¨å†Œ
AutoConfig.register("olmoe", OlmoeConfig)
AutoModelForCausalLM.register(OlmoeConfig, OlmoeForCausalLM)

# ç„¶åå¯ä»¥ä½¿ç”¨AutoModel
model = AutoModelForCausalLM.from_pretrained("allenai/OLMoE-1B-7B-0924")
```

## ğŸ¯ ä¸AlphaQuant GPTQé›†æˆ

æ‰€æœ‰è¿™äº›æ¨¡å‹éƒ½å·²ç»åœ¨AlphaQuantçš„GPTQ pipelineä¸­å¾—åˆ°æ”¯æŒï¼š

```python
from alphaquant.gptq import gptq_quantize_model, detect_moe_architecture
from transformers import AutoModelForCausalLM

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("allenai/OLMoE-1B-7B-0924")

# è‡ªåŠ¨æ£€æµ‹æ¶æ„
arch = detect_moe_architecture(model)  # è¿”å› 'olmoe'

# é‡åŒ–ï¼ˆè‡ªåŠ¨ä½¿ç”¨MoEä¼˜åŒ–ï¼‰
quantizers = gptq_quantize_model(
    model=model,
    dataloader=calib_data,
    layer_config=config,
    device='cuda'
)
```

## ğŸ” æ¨¡å‹æ¶æ„æ£€æµ‹

AlphaQuantä¼šè‡ªåŠ¨æ£€æµ‹ä»¥ä¸‹MoEæ¶æ„ï¼š

| æ¨¡å‹ | æ£€æµ‹å…³é”®è¯ | Top-K | ç‰¹æ®Šå¤„ç† |
|------|-----------|-------|----------|
| **OLMoE** | `olmoe`, `olmo` | 8 | Standard routing |
| **Qwen2-MoE** | `qwen`, `qwen2moe` | 4 | Shared expert gate |
| **Mixtral** | `mixtral` | 2 | Standard routing |
| **DeepSeek-MoE** | `deepseek` | Variable | Multi-level routing |

## ğŸ“ Importä¿®å¤

æ‰€æœ‰æ¨¡å‹æ–‡ä»¶å·²ç»ä¿®å¤äº†ä»transformersçš„ç›¸å¯¹å¯¼å…¥ï¼š

```python
# âŒ æ—§çš„ (ä»transformersåº“å¤åˆ¶)
from ...modeling_utils import PreTrainedModel
from ...configuration_utils import PretrainedConfig

# âœ… æ–°çš„ (ä¿®å¤å)
from transformers.modeling_utils import PreTrainedModel
from transformers.configuration_utils import PretrainedConfig
```

è¿™æ ·å³ä½¿å°†æ¥transformersåº“æ›´æ–°ï¼Œè¿™äº›æ¨¡å‹å®šä¹‰ä»ç„¶å¯ä»¥æ­£å¸¸å·¥ä½œã€‚

## ğŸ› ï¸ ä¾èµ–è¦æ±‚

åŸºæœ¬è¦æ±‚ï¼š
```
torch >= 2.0.0
transformers >= 4.36.0
```

å¯¹äºæŸäº›æ–°ç‰¹æ€§ï¼ˆå¦‚rope_utilsï¼‰ï¼Œå¯èƒ½éœ€è¦ï¼š
```
transformers >= 4.41.0
```

å¦‚æœtransformersç‰ˆæœ¬è¾ƒæ—§ï¼Œé…ç½®æ–‡ä»¶ä¼šè‡ªåŠ¨fallbackï¼š

```python
try:
    from transformers.modeling_rope_utils import RopeParameters
except ImportError:
    RopeParameters = None  # Fallback for older versions
```

## ğŸ“š æ¥æº

- **OLMoE**: ä»æœ€æ–°transformersåº“å¤åˆ¶ï¼ˆ4.46+ï¼‰
- **Qwen2-MoE**: ä»transformers 4.37+å¤åˆ¶
- **Mixtral**: ä»transformers 4.36+å¤åˆ¶
- **DeepSeek-MoE**: ä»MoEQuanté¡¹ç›®å¤åˆ¶

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ç‰ˆæœ¬å…¼å®¹æ€§**: è¿™äº›æ¨¡å‹å®šä¹‰ç‹¬ç«‹äºtransformersç‰ˆæœ¬ï¼Œä½†æŸäº›åŠŸèƒ½å¯èƒ½éœ€è¦æ–°ç‰ˆæœ¬transformersçš„ä¾èµ–
2. **å‘½åç©ºé—´**: ä½¿ç”¨`models.`å‰ç¼€å¯¼å…¥ä»¥é¿å…ä¸transformersåº“å†²çª
3. **æ›´æ–°**: å¦‚æœæ¨¡å‹å®šä¹‰æœ‰é‡å¤§æ›´æ–°ï¼Œéœ€è¦æ‰‹åŠ¨åŒæ­¥

## ğŸ”§ ç»´æŠ¤å’Œæ›´æ–°

å¦‚æœéœ€è¦æ·»åŠ æ–°æ¨¡å‹æˆ–æ›´æ–°ç°æœ‰æ¨¡å‹ï¼š

1. ä»transformersåº“æˆ–å®˜æ–¹repoå¤åˆ¶æœ€æ–°ç‰ˆæœ¬
2. ä¿®å¤importè·¯å¾„ï¼ˆä½¿ç”¨`fix_model_imports.py`è„šæœ¬ï¼‰
3. æ·»åŠ `__init__.py`
4. æ›´æ–°æ­¤README
5. åœ¨`alphaquant/gptq/model_utils.py`ä¸­æ·»åŠ æ£€æµ‹é€»è¾‘
6. åœ¨`alphaquant/gptq/gptq_moe.py`ä¸­æ·»åŠ MoEè·¯ç”±å¤„ç†ï¼ˆå¦‚æœæ˜¯MoEæ¨¡å‹ï¼‰

### ä½¿ç”¨ä¿®å¤è„šæœ¬

```bash
# ä¿®å¤æ‰€æœ‰æ¨¡å‹æ–‡ä»¶çš„import
python fix_model_imports.py
```

## ğŸ§ª æµ‹è¯•

æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```python
# è¿è¡Œæµ‹è¯•è„šæœ¬
python tests/test_model_loading.py
```

æˆ–æ‰‹åŠ¨æµ‹è¯•ï¼š

```python
from models.olmoe.modeling_olmoe import OlmoeForCausalLM

try:
    model = OlmoeForCausalLM.from_pretrained(
        "allenai/OLMoE-1B-7B-0924",
        torch_dtype=torch.bfloat16,
        device_map='cpu'
    )
    print("âœ“ OLMoEæ¨¡å‹åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"âœ— é”™è¯¯: {e}")
```

## ğŸ“– ç›¸å…³æ–‡æ¡£

- [GPTQ Pipeline README](../GPTQ_PIPELINE_README.md) - GPTQé‡åŒ–æ–‡æ¡£
- [MoE Support](../alphaquant/gptq/MOE_SUPPORT.md) - MoEä¸“ç”¨ä¼˜åŒ–æ–‡æ¡£
- [Transformers Documentation](https://huggingface.co/docs/transformers) - åŸå§‹transformersæ–‡æ¡£

